+++++++++++++++++++++++++++++++++++
Estructura de Archivos-----------------------
    |--core.py
    |--routes.py
    |--worker.py
|--data/
    |--estandar.json
    |--limites.json
    |--river_models/
|--rules/
    |--base.py
    |--rule1.py
    |--rule2.py
    |--rule3.py
    |--__init__.py

Contenido de Archivos-----------------------

core.py----------------------------------------
# app/iotelligence/core.py
from __future__ import annotations
from datetime import datetime, timezone
from typing import Optional
from app.iotelligence.worker import submit           # 👈 solo submit (NO init aquí)
from app.iotelligence.rules import REGISTRY
from app.models import Dispositivo

def dispatch_measure(dispositivo: Dispositivo, metric: str | None, value, ts: Optional[datetime] = None):
    """
    Router para eventos en tiempo real (MQTT/PUT).
    - metric puede ser None para reglas de configuración (Rule 2).
    """
    ts = ts or datetime.now(timezone.utc)
    for rule in REGISTRY.values():
        submit(rule.on_measure, dispositivo, metric, value, ts)

def run_rule_batch(rule_name: str, **kwargs):
    """
    Encola la ejecución batch de una regla (histórico / auditorías).
    El worker ya fue inicializado en app/__init__.py.
    """
    rule = REGISTRY.get(rule_name)
    if not rule:
        raise ValueError(f"Regla no registrada: {rule_name}")
    return submit(rule.run_batch, **kwargs)
routes.py----------------------------------------
# app/iotelligence/routes.py
from __future__ import annotations
from flask import Blueprint, request, jsonify, Response, stream_with_context
from queue import Empty
import json, time

from app.models import Dispositivo
from app.iotelligence.core import run_rule_batch      # <<< runner de reglas (concurrencia)
from app.sse import publish as sse_publish, subscribe, unsubscribe
from app.utils_time import now_utc, iso_local         # <<< AÑADIR

bp_ai = Blueprint("iotelligence", __name__)

@bp_ai.route("/ai/anomaly", methods=["POST"])
def ai_anomaly():
    """
    Lanza un análisis batch de Regla 1 (valores extremos) sobre una métrica histórica.
    Body:
      { "dispositivo_id": 1, "metric": "temperatura", "days": 7 }
    Respuesta inmediata: {"status":"queued"}
    Los hallazgos se publican por SSE en /stream/ai (eventos: ai_anomaly, ai_done).
    """
    data = request.get_json() or {}
    dispositivo_id = int(data.get("dispositivo_id", 0))
    metric = data.get("metric", "temperatura")
    days = int(data.get("days", 7))

    disp = Dispositivo.query.get(dispositivo_id)
    if not disp:
        return jsonify({"error": "dispositivo no existe"}), 404

    # Aviso de que se encoló el job (con timestamps local/UTC)
    ts_utc = now_utc()                                   # <<< NUEVO
    sse_publish({
        "event": "ai_progress",
        "status": "queued",
        "rule": "extremos",
        "dispositivo_id": dispositivo_id,
        "metric": metric,
        "window_days": days,
        "ts_local": iso_local(ts_utc),                   # <<< NUEVO
        "ts_utc": ts_utc.isoformat()                     # <<< NUEVO
    })

    # Ejecuta la Regla 1 en batch (histórico) en background
    run_rule_batch("extremos", dispositivo=disp, metric=metric, days=days)

    return jsonify({"status": "queued"}), 202


@bp_ai.route("/stream/ai", methods=["GET"])
def stream_ai():
    """
    SSE para eventos de IA:
      - ai_anomaly     (hallazgos)
      - ai_misconfig   (regla 2)
      - ai_fix_applied (si aplicas parches)
      - ai_done        (fin de job batch)
      - ai_progress    (progreso encolado)
    """
    def gen():
        q = subscribe()
        try:
            yield "event: hello\ndata: {}\n\n"
            last = time.time()
            while True:
                try:
                    evt = q.get(timeout=5)
                    # Solo eventos IA            
                    if isinstance(evt, dict) and str(evt.get("event", "")).startswith("ai_"):
                        payload = json.dumps(evt, ensure_ascii=False, separators=(',', ':'))
                        yield f"event: {evt.get('event')}\n"
                        yield f"data: {payload}\n\n"
                except Empty:
                    # keep-alive
                    if time.time() - last > 25:
                        yield "event: ping\ndata: {}\n\n"
                        last = time.time()
        finally:
            unsubscribe(q)

    headers = {
        "Content-Type": "text/event-stream; charset=utf-8",
        "Cache-Control": "no-cache",
        "X-Accel-Buffering": "no",
        "Connection": "keep-alive",
    }
    return Response(stream_with_context(gen()), headers=headers)
worker.py----------------------------------------
# app/iotelligence/worker.py
from __future__ import annotations
from concurrent.futures import ThreadPoolExecutor
from typing import Callable, Any
from flask import Flask

_executor: ThreadPoolExecutor | None = None
_app: Flask | None = None

def init(app: Flask, max_workers: int = 2) -> None:
    global _executor, _app
    _app = app
    if _executor is None:
        _executor = ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="AIWorker")
        print(f"[AI worker] started (max_workers={max_workers})")

def _run_with_app(fn: Callable[..., Any], *args, **kwargs) -> Any:
    if _app is None:
        return fn(*args, **kwargs)
    with _app.app_context():
        return fn(*args, **kwargs)

def submit(fn: Callable[..., Any], *args, **kwargs):
    if _executor is None:
        raise RuntimeError("AI worker no inicializado. Llama init(app) en create_app().")
    return _executor.submit(_run_with_app, fn, *args, **kwargs)
data\estandar.json----------------------------------------
{
  "LGT0": {
    "expected_modo": "horario",
    "intervalo_min_s": 1,
    "intervalo_max_s": 3600
  },
  "RGD0": {
    "expected_modo": "horario",
    "intervalo_min_s": 1,
    "intervalo_max_s": 3600
  },
  "SHD0": {
    "expected_modo": "horario",
    "intervalo_min_s": 1,
    "intervalo_max_s": 3600
  },
  "FAN0": {
    "expected_modo": "horario",
    "intervalo_min_s": 1,
    "intervalo_max_s": 3600
  },
  "PLG0": {
    "expected_modo": "horario",
    "intervalo_min_s": 1,
    "intervalo_max_s": 3600
  },
  "MOV0": {
    "expected_modo": "horario",
    "intervalo_min_s": 1,
    "intervalo_max_s": 3600
  }
}
data\limites.json----------------------------------------
{
  "TMP0": {
    "temperatura": {
      "min": 10.0,
      "max": 80.0
    },
    "humedad": {
      "min": 20,
      "max": 80
    }
  },
  "CO20": {
    "co2_ppm": {
      "min": 0,
      "max": 800
    }
  },
  "LUX0": {
    "luz_lux": {
      "min": 0,
      "max": 800
    }
  },
  "SND0": {
    "db": {
      "min": 0,
      "max": 100
    }
  },
  "PLG0": {
    "consumo_w": {
      "min": 0.0,
      "max": 2500.0
    }
  }
}

rules\base.py----------------------------------------
# app/iotelligence/rules/base.py
from __future__ import annotations
from typing import Optional, Dict, Any
from datetime import datetime

class Rule:
    name: str = "rule"

    def on_measure(self, dispositivo, metric: str, value, ts: datetime) -> None:
        """Procesa una sola medición (tiempo real)."""
        return

    def run_batch(self, **kwargs) -> Optional[Dict[str, Any]]:
        """Procesa en modo histórico/batch."""
        return None
rules\rule1.py----------------------------------------
# app/iotelligence/rules/rule1.py
from __future__ import annotations
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime, timedelta, timezone
import os, json, math, threading, time
from flask import current_app
from app.sse import publish as sse_publish
from app.models import Dispositivo, EstadoLog
from app.iotelligence.rules.base import Rule
from app.utils_time import now_utc, iso_local

# ======== DATA (solo limites.json) ========
_BASE_DIR = os.path.dirname(os.path.dirname(__file__))
_DATA_DIR = os.path.join(_BASE_DIR, "data")
_LIM_PATH = os.path.join(_DATA_DIR, "limites.json")

_LIMITS: Dict[str, Dict[str, Any]] = {}
_LIM_LOADED = False

def _default_limits() -> Dict[str, Dict[str, Dict[str, float]]]:
    return {
        "TMP0": {"temperatura": {"min":18.0,"max":32.0}, "humedad":{"min":20.0,"max":80.0}},
        "CO20": {"co2_ppm": {"min":350,"max":1200}},
        "LUX0": {"luz_lux": {"min":0.0,"max":500.0}},
        "SND0": {"db": {"min":30.0,"max":85.0}},
        "PLG0": {"consumo_w": {"min":0.0,"max":1500.0}},
        "FAN0": {"velocidad": {"min":0,"max":3}}
    }

def _ensure_and_load_limits():
    global _LIM_LOADED
    if _LIM_LOADED: return
    os.makedirs(_DATA_DIR, exist_ok=True)
    if not os.path.isfile(_LIM_PATH):
        with open(_LIM_PATH, "w", encoding="utf-8") as f:
            json.dump(_default_limits(), f, ensure_ascii=False, indent=2)
    try:
        with open(_LIM_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict):
            _LIMITS.update(data)
        else:
            _LIMITS.update(_default_limits())
    except Exception:
        _LIMITS.update(_default_limits())
    _LIM_LOADED = True

def _limits_for(serial: str) -> Dict[str, Any]:
    for pfx, rules in _LIMITS.items():
        if serial.startswith(pfx): 
            return rules
    return {}

def _bounds_limits(disp: Dispositivo, metric: str) -> Dict[str, Any]:
    mm = _limits_for(disp.serial_number or "").get(metric) or {}
    out: Dict[str, Any] = {}
    if isinstance(mm.get("min"), (int,float)): out["min"] = float(mm["min"])
    if isinstance(mm.get("max"), (int,float)): out["max"] = float(mm["max"])
    if out: out["source"] = "limits"
    return out

# =========================
# Series y percentiles hist
# =========================
def _fetch_series(disp_id: int, metric: str, since, until, limit=5000) -> List[Tuple[datetime, float]]:
    q = EstadoLog.query.filter_by(dispositivo_id=disp_id)
    q = q.filter(EstadoLog.timestamp >= since).filter(EstadoLog.timestamp <= until)
    q = q.order_by(EstadoLog.timestamp.asc())
    out: List[Tuple[datetime, float]] = []
    for log in q.limit(limit).all():
        v = (log.parametros or {}).get(metric)
        if isinstance(v,(int,float)):
            out.append((log.timestamp, float(v)))
    return out

def _percentile(sorted_vals: List[float], p: float) -> float:
    if not sorted_vals: return math.nan
    k = (len(sorted_vals)-1) * (p/100.0)
    f = math.floor(k); c = math.ceil(k)
    if f == c: return sorted_vals[int(k)]
    return sorted_vals[f]*(c-k) + sorted_vals[c]*(k-f)

def _hist_bounds(series: List[Tuple[datetime,float]]) -> Dict[str, Any]:
    if not series: return {}
    vals = sorted(v for _, v in series)
    pmin = float(current_app.config.get("AI_HIST_PMIN", 1.0))
    pmax = float(current_app.config.get("AI_HIST_PMAX", 99.0))
    lo = _percentile(vals, pmin); hi = _percentile(vals, pmax)
    if math.isnan(lo) or math.isnan(hi) or lo >= hi: return {}
    pad_frac = float(current_app.config.get("AI_HIST_PAD_FRAC", 0.05))
    pad_abs  = float(current_app.config.get("AI_HIST_PAD_ABS", 0.0))
    span = hi - lo
    pad = max(pad_abs, span*pad_frac) if span>0 else pad_abs
    return {"min": lo - pad, "max": hi + pad, "source": "hist"}

# ===============
# Fusión de bounds
# ===============
def _fuse(*bounds_list: Dict[str, Any]) -> Dict[str, Any]:
    mins, maxs, srcs = [], [], []
    for b in bounds_list:
        if not b: continue
        if "min" in b: mins.append(b["min"])
        if "max" in b: maxs.append(b["max"])
        if b.get("source"): srcs.append(b["source"])
    if not mins and not maxs: return {}
    out: Dict[str, Any] = {}
    if mins: out["min"] = max(mins)
    if maxs: out["max"] = min(maxs)
    if srcs: out["source"] = "+".join(srcs)
    return out

# ===============
# Cooldown (antispam)
# ===============
_LOCK = threading.Lock()
_LAST: Dict[tuple[int,str], float] = {}

def _throttle(disp_id: int, metric: str) -> bool:
    cd = int(current_app.config.get("AI_ALERT_COOLDOWN_S", 60))
    now = time.time()
    key = (disp_id, metric)
    with _LOCK:
        last = _LAST.get(key)
        if last and (now - last) < cd:
            return True
        _LAST[key] = now
        return False

# ===============
# Regla 1
# ===============
class Rule1Extremos(Rule):
    name = "extremos"

    def on_measure(self, dispositivo, metric: str, value, ts: datetime) -> None:
        # Solo dispositivos reclamados y métricas numéricas
        if not getattr(dispositivo, "reclamado", False):
            return
        if not isinstance(value, (int,float)):
            return

        _ensure_and_load_limits()
        if _throttle(dispositivo.id, metric):
            return

        # Bounds: limites.json + histórico (si hay suficientes puntos)
        b_lim = _bounds_limits(dispositivo, metric)

        days = int(current_app.config.get("AI_HIST_WINDOW_DAYS", 30))
        until = datetime.now(timezone.utc)
        since = until - timedelta(days=days)
        series = _fetch_series(dispositivo.id, metric, since, until)
        b_hist = {}
        if len(series) >= int(current_app.config.get("AI_HIST_MIN_POINTS", 500)):
            b_hist = _hist_bounds(series)

        bounds = _fuse(b_lim, b_hist) or b_lim or b_hist
        if not bounds:
            return

        mn = bounds.get("min"); mx = bounds.get("max")
        if mn is None and mx is None:
            return

        span = (mx - mn) if (mn is not None and mx is not None and mx>mn) else 0.0
        tol = max(
            float(current_app.config.get("AI_ALERT_TOL_ABS", 0.5)),
            span * float(current_app.config.get("AI_ALERT_TOL_FRAC", 0.02))
        )
        low  = (mn is not None) and (value < (mn - tol))
        high = (mx is not None) and (value > (mx + tol))
        if not (low or high):
            return

        ts_utc = ts or now_utc()
        sse_publish({
            "event":"ai_anomaly","rule":self.name,
            "dispositivo_id":dispositivo.id,
            "serial_number":dispositivo.serial_number,
            "metric":metric,"value":value,"bounds":bounds,
            "ts_local": iso_local(ts_utc), # hora local
            "ts_utc": ts_utc.isoformat()    # hora UTC
        })

    def run_batch(self, dispositivo, metric: str, days: int = 7):
        """
        Recorre el histórico de 'days' días, calcula bounds fusionados y
        emite ai_anomaly por cada valor fuera de rango. Al final emite ai_done.
        """
        if not getattr(dispositivo, "reclamado", False):
            sse_publish({
                "event": "ai_done",
                "rule": self.name,
                "result": {"metric": metric, "window_days": days, "found": 0, "skipped": "unclaimed"}
            })
            return {"found": 0, "skipped": "unclaimed"}

        _ensure_and_load_limits()

        b_lim = _bounds_limits(dispositivo, metric)

        until = datetime.now(timezone.utc)
        since = until - timedelta(days=int(days))
        series = _fetch_series(dispositivo.id, metric, since, until)

        b_hist = {}
        if len(series) >= int(current_app.config.get("AI_HIST_MIN_POINTS", 500)):
            b_hist = _hist_bounds(series)

        bounds = _fuse(b_lim, b_hist) or b_lim or b_hist
        if not bounds:
            sse_publish({
                "event": "ai_done",
                "rule": self.name,
                "result": {"metric": metric, "window_days": days, "found": 0, "reason": "no_bounds"}
            })
            return {"found": 0, "reason": "no_bounds"}

        mn = bounds.get("min"); mx = bounds.get("max")
        span = (mx - mn) if (mn is not None and mx is not None and mx > mn) else 0.0
        tol = max(
            float(current_app.config.get("AI_ALERT_TOL_ABS", 0.5)),
            span * float(current_app.config.get("AI_ALERT_TOL_FRAC", 0.02))
        )

        found = 0
        for ts, val in series:
            if not isinstance(val, (int, float)):
                continue
            low  = (mn is not None) and (val < (mn - tol))
            high = (mx is not None) and (val > (mx + tol))
            if not (low or high):
                continue
            found += 1

            ts_utc = ts or now_utc()

            sse_publish({
                "event": "ai_anomaly",
                "rule": self.name,
                "dispositivo_id": dispositivo.id,
                "serial_number": dispositivo.serial_number,
                "metric": metric,
                "value": val,
                "bounds": bounds,
                "ts_local": iso_local(ts_utc), # hora local
                "ts_utc": ts_utc.isoformat()    # hora UTC
            })

        sse_publish({
            "event": "ai_done",
            "rule": self.name,
            "result": {"metric": metric, "window_days": days, "found": found}
        })
        return {"found": found, "metric": metric, "window_days": days}
rules\rule2.py----------------------------------------
# app/iotelligence/rules/rule2.py
from __future__ import annotations
from typing import Dict, Any, Optional
from datetime import datetime
import os, json, threading, time
from flask import current_app
from app.sse import publish as sse_publish
from app.models import Dispositivo
from app.iotelligence.rules.base import Rule
from app.utils_time import now_utc, iso_local

# ========= CARGA DE estandar.json (solo modo + intervalo + cooldown) =========
_BASE_DIR = os.path.dirname(os.path.dirname(__file__))
_DATA_DIR = os.path.join(_BASE_DIR, "data")
_STD_PATH = os.path.join(_DATA_DIR, "estandar.json")

# Simplificado: expected_modo, límites de intervalo_envio y cooldown por prefijo
_DEFAULT_STD: Dict[str, Dict[str, Any]] = {
    "LGT0": {"expected_modo": "horario", "intervalo_min_s": 1, "intervalo_max_s": 3600, "misconfig_cooldown_s": 3600},
    "RGD0": {"expected_modo": "horario", "intervalo_min_s": 1, "intervalo_max_s": 3600, "misconfig_cooldown_s": 3600},
    "SHD0": {"expected_modo": "horario", "intervalo_min_s": 1, "intervalo_max_s": 3600, "misconfig_cooldown_s": 3600},
    "FAN0": {"expected_modo": "horario", "intervalo_min_s": 1, "intervalo_max_s": 3600, "misconfig_cooldown_s": 3600},
    "PLG0": {"expected_modo": "horario", "intervalo_min_s": 1, "intervalo_max_s": 3600, "misconfig_cooldown_s": 3600},
    "MOV0": {"expected_modo": "horario", "intervalo_min_s": 1, "intervalo_max_s": 3600, "misconfig_cooldown_s": 3600}
}

_STD: Dict[str, Dict[str, Any]] = {}
_STD_LOADED = False

def _ensure_and_load_std():
    global _STD_LOADED
    if _STD_LOADED:
        return
    os.makedirs(_DATA_DIR, exist_ok=True)
    if not os.path.isfile(_STD_PATH):
        with open(_STD_PATH, "w", encoding="utf-8") as f:
            json.dump(_DEFAULT_STD, f, ensure_ascii=False, indent=2)
    try:
        with open(_STD_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict):
            _STD.update(data)
        else:
            _STD.update(_DEFAULT_STD)
    except Exception:
        _STD.update(_DEFAULT_STD)
    _STD_LOADED = True

def _std_for(serial: str) -> Optional[Dict[str, Any]]:
    for pfx, rules in _STD.items():
        if serial.startswith(pfx):
            return rules
    return None

# ========= ESTADO EN MEMORIA + COOLDOWN / REMINDERS =========
_STATE_LOCK = threading.Lock()
_MISCONFIG_STATE: Dict[int, bool] = {}   # True si está en misconfig; False/None si OK
_LAST_NOTIFY_TS: Dict[int, float] = {}   # última notificación por dispositivo (epoch)

def _detect_cooldown_global() -> int:
    """Cooldown global por config; fallback 3600s (para la PRIMERA alerta)."""
    try:
        return int(current_app.config.get("AI_MISCONFIG_COOLDOWN_S", 3600))
    except Exception:
        return 3600

def _detect_cooldown_for(disp: Dispositivo) -> int:
    """
    Si el estandar del prefijo define 'misconfig_cooldown_s', lo usa.
    Si no, usa el global AI_MISCONFIG_COOLDOWN_S.
    """
    try:
        std = _std_for(disp.serial_number or "")
        if std and isinstance(std.get("misconfig_cooldown_s"), (int, float)):
            return int(std["misconfig_cooldown_s"])
    except Exception:
        pass
    return _detect_cooldown_global()

def _remind_enabled() -> bool:
    return bool(current_app.config.get("AI_MISCONFIG_REMIND", False))

def _remind_cooldown_s() -> int:
    try:
        return int(current_app.config.get("AI_MISCONFIG_REMIND_COOLDOWN_S", 900))
    except Exception:
        return 900

def _should_notify(disp_id: int, new_state_is_misconfig: bool, detect_cd_s: int) -> bool:
    """
    - OK -> MISCONFIG: notifica si pasó detect_cd_s (antirrebote).
    - Sigue en MISCONFIG: si REMIND=True, re-notifica cada remind_cooldown.
    - MISCONFIG -> OK: resetea estado (no notifica).
    """
    now = time.time()
    detect_cd = int(detect_cd_s)
    remind_cd = _remind_cooldown_s()
    remind_on = _remind_enabled()

    with _STATE_LOCK:
        prev_state = _MISCONFIG_STATE.get(disp_id, False)
        last_any = _LAST_NOTIFY_TS.get(disp_id, 0.0)

        if new_state_is_misconfig:
            if not prev_state:
                # transición OK -> MISCONFIG
                if now - last_any >= detect_cd:
                    _MISCONFIG_STATE[disp_id] = True
                    _LAST_NOTIFY_TS[disp_id] = now
                    return True
                _MISCONFIG_STATE[disp_id] = True
                return False
            else:
                # persiste en MISCONFIG → ¿recordatorio?
                if remind_on and (now - last_any >= remind_cd):
                    _LAST_NOTIFY_TS[disp_id] = now
                    return True
                return False
        else:
            # MISCONFIG -> OK (o se mantiene OK)
            if prev_state:
                _MISCONFIG_STATE[disp_id] = False
            return False

# ========= LÓGICA DE EVALUACIÓN (solo modo + intervalo_envio) =========
def _evaluate(dispositivo: Dispositivo) -> Dict[str, Any]:
    """
    Devuelve {"issues":[...], "patch":{...}, "severity":"low"} o {} si OK.
    - Solo dispositivos RECLAMADOS (el caller ya filtra).
    - Solo evalúa configuracion: 'modo' y 'intervalo_envio'.
    """
    _ensure_and_load_std()

    cfg = dispositivo.configuracion or {}
    std = _std_for(dispositivo.serial_number or "")
    if not std:
        return {}

    issues = []
    patch_cfg: Dict[str, Any] = {}

    # 1) modo esperado = "horario"
    expected_modo = str(std.get("expected_modo", "")).lower()
    modo = str(cfg.get("modo", "")).lower()
    if expected_modo and modo != expected_modo:
        issues.append(f"config.modo = '{modo or 'N/A'}' → se recomienda '{expected_modo}'")
        patch_cfg["modo"] = expected_modo

    # 2) intervalo_envio dentro de [min,max]
    try:
        intervalo = int(cfg.get("intervalo_envio", 0))
    except Exception:
        intervalo = 0  # fuerza a entrar al ajuste

    min_s = int(std.get("intervalo_min_s", 1))
    max_s = int(std.get("intervalo_max_s", 3600))
    if intervalo < min_s or intervalo > max_s:
        issues.append(f"config.intervalo_envio fuera de rango [{min_s},{max_s}]")
        # sugerimos ajustar al rango sin inventar un valor fijo externo
        patch_cfg["intervalo_envio"] = max(min(intervalo or min_s, max_s), min_s)

    if not issues:
        return {}

    return {
        "issues": issues,
        "patch": {"configuracion": patch_cfg} if patch_cfg else {},
        "severity": "low"
    }

# ========= RULE 2 =========
class Rule2Misconfig(Rule):
    name = "misconfig"

    def applies_realtime(self, disp, metric, value) -> bool:
        # Aplica a dispositivos reclamados; no depende de métricas.
        return getattr(disp, "reclamado", False)

    def on_measure(self, dispositivo: Dispositivo, metric: str, value, ts: datetime) -> None:
        if not getattr(dispositivo, "reclamado", False):
            return

        res = _evaluate(dispositivo)
        is_misconfig = bool(res)
        detect_cd = _detect_cooldown_for(dispositivo)

        if _should_notify(dispositivo.id, is_misconfig, detect_cd) and is_misconfig:
            ts_utc = ts or now_utc()
            sse_publish({
                "event": "ai_misconfig",
                "rule": self.name,
                "dispositivo_id": dispositivo.id,
                "serial_number": dispositivo.serial_number,
                "issues": res.get("issues", []),
                "suggested_patch": res.get("patch", {}),
                "severity": res.get("severity", "low"),
                "ts_local": iso_local(ts_utc),
                "ts_utc": ts_utc.isoformat()
            })

    def run_batch(self, dispositivo: Dispositivo, **kwargs):
        # Batch opcional para evaluar una vez bajo demanda (misma lógica de notify)
        if not getattr(dispositivo, "reclamado", False):
            return {"skipped": "unclaimed"}

        res = _evaluate(dispositivo)
        is_misconfig = bool(res)
        detect_cd = _detect_cooldown_for(dispositivo)

        if _should_notify(dispositivo.id, is_misconfig, detect_cd) and is_misconfig:
            ts_utc = now_utc()
            sse_publish({
                "event": "ai_misconfig",
                "rule": self.name,
                "dispositivo_id": dispositivo.id,
                "serial_number": dispositivo.serial_number,
                "issues": res.get("issues", []),
                "suggested_patch": res.get("patch", {}),
                "severity": res.get("severity", "low"),
                "ts_local": iso_local(ts_utc),
                "ts_utc": ts_utc.isoformat()
            })
        return {"misconfig": is_misconfig}
rules\rule3.py----------------------------------------
# app/iotelligence/rules/rule3.py
from __future__ import annotations
from typing import Dict, Any, List, Tuple, Optional
from datetime import datetime
import os, time, pickle
from flask import current_app

from app.iotelligence.rules.base import Rule
from app.sse import publish as sse_publish
from app.models import Dispositivo
from app.utils_time import now_utc, iso_local, to_local

# ---- River (IA online) ----
from river import linear_model, optim, compose, preprocessing

# ===== Persistencia de modelos por dispositivo (PICKLE) =====
def _model_dir() -> str:
    base = current_app.config.get("AI_R3_MODEL_DIR", "app/iotelligence/data/river_models")
    os.makedirs(base, exist_ok=True)
    return base

def _model_path(serial: str) -> str:
    safe = "".join(ch for ch in (serial or "unknown") if ch.isalnum() or ch in ("-","_"))
    return os.path.join(_model_dir(), f"{safe}.river.pkl")  # <<< usa .pkl

def _new_model():
    # Features numéricas -> StandardScaler + LogisticRegression
    return compose.Pipeline(
        ("scale", preprocessing.StandardScaler()),
        ("lr", linear_model.LogisticRegression(optimizer=optim.SGD(0.05)))
    )

def _load_model(serial: str):
    path = _model_path(serial)
    warm = bool(current_app.config.get("AI_R3_WARM_START", True))
    if warm and os.path.isfile(path):
        try:
            with open(path, "rb") as f:
                return pickle.load(f)
        except Exception:
            pass
    return _new_model()

def _save_model(serial: str, model):
    try:
        with open(_model_path(serial), "wb") as f:
            pickle.dump(model, f)
    except Exception:
        pass

# ===== Features de tiempo =====
def _time_features(ts_utc: datetime) -> Dict[str, Any]:
    loc = to_local(ts_utc)
    return {
        "hora": loc.hour,
        "minuto": loc.minute,
        "dia_semana": loc.weekday(),        # 0=Lunes .. 6=Domingo
        "es_fin_semana": 1 if loc.weekday() >= 5 else 0
    }

def _bins_per_day() -> int:
    step = int(current_app.config.get("AI_R3_BIN_MINUTES", 15))
    return max(1, 24*60 // step)

def _bin_edges(step_min: int) -> List[Tuple[int, int]]:
    edges = []
    total = 24*60
    for start in range(0, total, step_min):
        edges.append((start, min(start+step_min, total)))
    return edges

def _features_for_bin(bin_idx: int, weekday: int) -> Dict[str, Any]:
    step = int(current_app.config.get("AI_R3_BIN_MINUTES", 15))
    start_min = bin_idx * step
    mid_min = start_min + step//2
    h = mid_min // 60
    m = mid_min % 60
    return {
        "hora": h,
        "minuto": m,
        "dia_semana": weekday,
        "es_fin_semana": 1 if weekday >= 5 else 0
    }

def _merge_on_windows(mask: List[bool], step_min: int, min_bins: int) -> List[Tuple[str, str]]:
    def mm_to_hhmm(mm: int) -> str:
        h = mm // 60; m = mm % 60
        return f"{h:02d}:{m:02d}"

    edges = _bin_edges(step_min)
    windows = []
    i = 0
    n = len(mask)
    while i < n:
        if not mask[i]:
            i += 1; continue
        j = i
        while j < n and mask[j]:
            j += 1
        length = j - i
        if length >= min_bins:
            start_min = edges[i][0]
            end_min   = edges[j-1][1]
            windows.append((mm_to_hhmm(start_min), mm_to_hhmm(end_min)))
        i = j
    return windows

# ===== Utilidades de horario actual =====
_DIAS = ["lunes","martes","miercoles","jueves","viernes","sabado","domingo"]

def _parse_hhmm(s: str) -> Optional[int]:
    try:
        h, m = s.strip().split(":")
        h = int(h); m = int(m)
        if 0 <= h <= 23 and 0 <= m <= 59:
            return h*60 + m
    except Exception:
        pass
    return None

def _current_schedule_masks(cfg: Dict[str, Any], step_min: int) -> Dict[int, List[bool]]:
    horarios = cfg.get("horarios")
    if not isinstance(horarios, list) or not horarios:
        return {}
    bins = 24*60 // step_min
    masks: Dict[int, List[bool]] = {wd: [False]*bins for wd in range(7)}
    for item in horarios:
        if not isinstance(item, dict):
            continue
        dias = item.get("dias")
        ini = _parse_hhmm(str(item.get("inicio", "")))
        fin = _parse_hhmm(str(item.get("fin", "")))
        if not isinstance(dias, list) or ini is None or fin is None:
            continue
        ranges = []
        if fin >= ini:
            ranges.append((ini, fin))
        else:
            ranges.append((ini, 24*60))
            ranges.append((0, fin))
        for dname in dias:
            dname_l = str(dname).lower()
            if dname_l not in _DIAS:
                continue
            wd = _DIAS.index(dname_l)
            for (a, b) in ranges:
                start_bin = a // step_min
                end_bin_excl = min((b + step_min - 1) // step_min, bins)
                for bi in range(start_bin, end_bin_excl):
                    masks[wd][bi] = True
    return {wd: m for wd, m in masks.items() if any(m)}

def _learned_masks(model, step_min: int, thresh: float) -> Dict[int, List[bool]]:
    bins = 24*60 // step_min
    out: Dict[int, List[bool]] = {}
    for wd in range(7):
        mask = []
        for b in range(bins):
            xf = _features_for_bin(b, wd)
            try:
                proba = model.predict_proba_one(xf)
                p_on = float(proba.get(True, 0.0))
            except Exception:
                p_on = 0.0
            mask.append(p_on >= thresh)
        if any(mask):
            out[wd] = mask
    return out

def _diff_ratio(m1: Dict[int, List[bool]], m2: Dict[int, List[bool]]) -> float:
    union = 0
    xor = 0
    for wd in range(7):
        a = m1.get(wd, [])
        b = m2.get(wd, [])
        n = max(len(a), len(b))
        if n == 0:
            continue
        aa = a + [False]*(n - len(a))
        bb = b + [False]*(n - len(b))
        for i in range(n):
            u = aa[i] or bb[i]
            x = (aa[i] != bb[i])
            if u: union += 1
            if x: xor += 1
    if union == 0:
        return 0.0
    return xor / float(union)

def _masks_to_windows_per_day(masks: Dict[int, List[bool]], step_min: int, min_bins: int) -> Dict[str, List[Dict[str,str]]]:
    out: Dict[str, List[Dict[str,str]]] = {}
    for wd, mask in masks.items():
        wins = _merge_on_windows(mask, step_min, min_bins)
        if wins:
            out[_DIAS[wd]] = [{"inicio": a, "fin": b} for a, b in wins]
    return out

# ===== Estado en memoria (por dispositivo) =====
_UPDATE_COUNT: Dict[int, int] = {}
_TOTAL_EVENTS: Dict[int, int] = {}
_LAST_SUGGEST_TS: Dict[int, float] = {}
_RESET_DONE: Dict[str, bool] = {}

def _maybe_reset_model_once(serial: str):
    if not bool(current_app.config.get("AI_R3_RESET_ON_START", False)):
        return
    if _RESET_DONE.get(serial):
        return
    path = _model_path(serial)
    try:
        if os.path.isfile(path):
            os.remove(path)
    except Exception:
        pass
    _RESET_DONE[serial] = True

def _should_emit_suggest(disp_id: int) -> bool:
    cd = int(current_app.config.get("AI_R3_COOLDOWN_S", 3600))
    min_events = int(current_app.config.get("AI_R3_MIN_EVENTS", 100))
    last = _LAST_SUGGEST_TS.get(disp_id, 0.0)
    now = time.time()
    if _TOTAL_EVENTS.get(disp_id, 0) < min_events:
        return False
    if now - last < cd:
        return False
    return True

def _maybe_save_model(serial: str, disp_id: int, model):
    n = int(current_app.config.get("AI_R3_SAVE_EVERY_N", 50))
    cnt = _UPDATE_COUNT.get(disp_id, 0)
    if cnt >= n:
        _save_model(serial, model)
        _UPDATE_COUNT[disp_id] = 0

def _emit_suggestion(dispositivo: Dispositivo, model, *, current_masks: Optional[Dict[int,List[bool]]] = None, diff_ratio_val: Optional[float] = None):
    step = int(current_app.config.get("AI_R3_BIN_MINUTES", 15))
    thresh = float(current_app.config.get("AI_R3_PROB_THRESH", 0.60))
    min_bins = int(current_app.config.get("AI_R3_MIN_SPAN_BINS", 2))

    learned_masks = _learned_masks(model, step, thresh)
    suggested_horarios = _masks_to_windows_per_day(learned_masks, step, min_bins)

    payload = {
        "event": "ai_suggest",
        "rule": self.name,
        "dispositivo_id": dispositivo.id,
        "serial_number": dispositivo.serial_number,
        "suggested_horarios": suggested_horarios,
        "bin_minutes": step,
        "threshold": thresh,
        "ts_local": iso_local(now_utc()),
    }
    if current_masks is not None:
        payload["current_horarios"] = _masks_to_windows_per_day(current_masks, step, min_bins)
    if diff_ratio_val is not None:
        payload["diff_ratio"] = diff_ratio_val

    sse_publish(payload)
    _LAST_SUGGEST_TS[dispositivo.id] = time.time()

class Rule3LearnSchedule(Rule):
    name = "learn"

    def applies_realtime(self, disp, metric, value) -> bool:
        return True

    def _target_from_dispositivo(self, dispositivo: Dispositivo) -> Optional[int]:
        cfg = dispositivo.configuracion or {}
        if isinstance(cfg.get("encendido"), bool):
            return 1 if cfg["encendido"] else 0
        st = str(dispositivo.estado or "").lower()
        if st in ("activo", "on", "true", "1"):
            return 1
        if st in ("inactivo", "off", "false", "0"):
            return 0
        return None

    def on_measure(self, dispositivo: Dispositivo, metric: str, value, ts: datetime) -> None:
        if not getattr(dispositivo, "reclamado", False):
            return

        # ===== Depuración inicial =====
        print("\n[R3] ===== Rule3 Trigger =====")
        print(f"[R3] Dispositivo ID: {dispositivo.id}")
        print(f"[R3] Serial: {dispositivo.serial_number}")
        print(f"[R3] Reclamado: {dispositivo.reclamado}")

        serial = dispositivo.serial_number or ""
        model_path = _model_path(serial)
        print(f"[R3] Ruta del modelo: {model_path} -> {'EXISTE' if os.path.isfile(model_path) else 'NO EXISTE'}")

        _maybe_reset_model_once(serial)

        target = self._target_from_dispositivo(dispositivo)
        print(f"[R3] Target detectado: {target}")
        if target is None:
            print("[R3] Target=None → no se entrena.")
            return

        # Cargar/crear modelo
        model = _load_model(serial)

        ts_utc = ts or now_utc()
        x = _time_features(ts_utc)
        try:
            model.learn_one(x, bool(target))   # no reasignar
            print(f"[R3] learn_one OK con x={x}")
        except Exception as e:
            print(f"[R3] learn_one ERROR: {e}")
            return

        disp_id = dispositivo.id
        _TOTAL_EVENTS[disp_id] = _TOTAL_EVENTS.get(disp_id, 0) + 1
        _UPDATE_COUNT[disp_id] = _UPDATE_COUNT.get(disp_id, 0) + 1
        print(f"[R3] Total events: {_TOTAL_EVENTS[disp_id]} | Updates desde último save: {_UPDATE_COUNT[disp_id]}")

        _maybe_save_model(serial, disp_id, model)

        # ¿Emitimos sugerencia?
        if not _should_emit_suggest(disp_id):
            # Explica por qué no
            cd = int(current_app.config.get('AI_R3_COOLDOWN_S', 3600))
            min_events = int(current_app.config.get('AI_R3_MIN_EVENTS', 100))
            last = _LAST_SUGGEST_TS.get(disp_id, 0.0)
            now_ts = time.time()
            faltan = max(0, min_events - _TOTAL_EVENTS.get(disp_id, 0))
            espera = max(0, cd - (now_ts - last))
            print(f"[R3] No sugiere aún → faltan_eventos={faltan}, cooldown_restante={espera:.1f}s")
            return

        cfg = dispositivo.configuracion or {}
        print(f"[R3] Configuración actual: {cfg}")
        print(f"[R3] Estado actual: {dispositivo.estado}")

        modo = str(cfg.get("modo", "")).lower()
        audit_when_horario = bool(current_app.config.get("AI_R3_AUDIT_WHEN_HORARIO", True))
        diff_thresh = float(current_app.config.get("AI_R3_DIFF_THRESH", 0.30))

        if audit_when_horario and modo == "horario":
            step = int(current_app.config.get("AI_R3_BIN_MINUTES", 15))
            thresh = float(current_app.config.get("AI_R3_PROB_THRESH", 0.60))
            current_masks = _current_schedule_masks(cfg, step)
            learned_masks = _learned_masks(model, step, thresh)
            diff = _diff_ratio(current_masks, learned_masks)
            print(f"[R3] Audit horario ON → diff_ratio={diff:.3f} (umbral={diff_thresh})")
            if diff >= diff_thresh:
                _save_model(serial, model)
                print("[R3] >>> Emite ai_suggest (horario, supera umbral)")
                _emit_suggestion(dispositivo, model, current_masks=current_masks, diff_ratio_val=diff)
            else:
                print("[R3] No emite ai_suggest (diff por debajo del umbral)")
        else:
            _save_model(serial, model)
            print("[R3] >>> Emite ai_suggest (no en horario o auditoría desactivada)")
            _emit_suggestion(dispositivo, model)
rules\__init__.py----------------------------------------
# app/iotelligence/rules/__init__.py
from .rule1 import Rule1Extremos
from .rule2 import Rule2Misconfig
from .rule3 import Rule3LearnSchedule

# registro disponible
REGISTRY = {
    "extremos": Rule1Extremos(),
    "misconfig": Rule2Misconfig(),
    "learn": Rule3LearnSchedule(),      # <<< nuevo
}

